{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator_bleu_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hloYMEtped9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helper\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_utils import batch_data, train_rnn\n",
        "from models import RNN\n",
        "import time\n",
        "import os\n",
        "import os\n",
        "import pickle\n",
        "# hyperparameters\n",
        "# 3, 5 ,7\n",
        "sequence_length = 3\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 0.002\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbGT_Fv7ey5t",
        "colab_type": "code",
        "outputId": "7479f0ef-3c90-47d9-ba2e-4b7cae17d9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import importlib\n",
        "importlib.reload(helper)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'helper' from '/content/helper.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-OCncoe056",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# int_text, vocab_to_int, int_to_vocab = helper.load_preprocess()\n",
        "int_text, vocab_to_int, int_to_vocab = pickle.load(open('preprocess_wv.pkl', mode='rb'))\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
        "vocab_size = len(vocab_to_int)\n",
        "output_size = len(vocab_to_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIByvVY1e5Z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.25)\n",
        "# if train_on_gpu:\n",
        "#     rnn.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F3Mh35ce7NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import helper\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "\n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "\n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "\n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "\n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)\n",
        "        current_seq = current_seq.cpu().data.numpy()\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "\n",
        "    listOfPeople = ['howard','sheldon', 'penny', 'raj', 'bernadette', 'amy', 'leonard']\n",
        "    for i in range(1, len(predicted)):\n",
        "      if predicted[i] in listOfPeople:\n",
        "        predicted[i-1] = predicted[i]+\".\"\n",
        "    gen_sentences = ' '.join(predicted)\n",
        "\n",
        "    # Replace punctuation tokens\n",
        "    # for key, token in token_dict.items():\n",
        "    #     ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "    #     gen_sentences = gen_sentences.replace(' ' + token, key)\n",
        "    # gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    # gen_sentences = gen_sentences.replace('( ', '(')\n",
        "\n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYvPvB9e-ol",
        "colab_type": "code",
        "outputId": "06d4d36f-91d5-4676-f27d-a98725022288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4e567fe560d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/helper.py\u001b[0m in \u001b[0;36mload_preprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mLoad\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPreprocessed\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mthem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocess.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocess.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zhOEXugKz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import random\n",
        "def selfBleu(generated_script):\n",
        "    removeScene = generated_script.index(\".\")\n",
        "    dialogue = generated_script[removeScene+1:]\n",
        "    removeNames = []\n",
        "    splitByWords = dialogue.split(\" \")\n",
        "    for i in range(0, len(splitByWords)-1):\n",
        "      if \":\" not in splitByWords[i]:\n",
        "        removeNames.append(splitByWords[i])\n",
        "    # print(removeNames)\n",
        "    sentences = \" \".join(removeNames).split(\".\")\n",
        "    wordSentences = []\n",
        "    for i in sentences:\n",
        "      tempList = i.split(\" \")\n",
        "      wordSentences.append(tempList[1:])\n",
        "    # print()\n",
        "    # print(wordSentences)\n",
        "\n",
        "    # generated_script\n",
        "    totalScore = 0.0\n",
        "    for i in wordSentences:\n",
        "      temp = list(wordSentences)\n",
        "      temp.remove(i)\n",
        "      # print(temp)\n",
        "      # print(i)\n",
        "      score = sentence_bleu(temp, i)\n",
        "      totalScore += score\n",
        "    returnScore = totalScore/len(wordSentences)\n",
        "    print(returnScore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUrUMtuKgO3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleu(generated_script):\n",
        "    tempDict = helper.load_data(\"./data.pkl\")\n",
        "    keyList = list(tempDict.keys())\n",
        "    episodeList = []\n",
        "    removeScene = generated_script.index(\".\")\n",
        "    dialogue = generated_script[removeScene+1:]\n",
        "    removeNames = []\n",
        "    splitByWords = dialogue.split(\" \")\n",
        "    for i in range(0, len(splitByWords)-1):\n",
        "      if \":\" not in splitByWords[i]:\n",
        "        removeNames.append(splitByWords[i])\n",
        "    # print(removeNames)\n",
        "    sentences = \" \".join(removeNames).split(\".\")\n",
        "    for h in keyList[:10]:\n",
        "      finalList = []\n",
        "      for i in tempDict[h]:\n",
        "        if \"scene:\" not in i.lower():\n",
        "          indexColon = i.index(\":\")\n",
        "          tempDialogue = i.lower()[indexColon+1:]\n",
        "          if \"(\" in tempDialogue:\n",
        "            tempDialogue = tempDialogue[:tempDialogue.index(\"(\")]\n",
        "          finalList.append(tempDialogue)\n",
        "      sentencesFinal = []\n",
        "      for j in finalList:\n",
        "        sentencesFinal.extend(j.split(\".\"))\n",
        "      wordOFSentence = []\n",
        "      for i in sentencesFinal:\n",
        "        wordOFSentence.append(i.split(\" \")[1:])\n",
        "      episodeList.extend(wordOFSentence)\n",
        "    wordSentences = []\n",
        "    for i in sentences:\n",
        "      tempList = i.split(\" \")\n",
        "      wordSentences.append(tempList[1:])\n",
        "    totalScore = 0.0\n",
        "    for i in wordSentences:\n",
        "      score = sentence_bleu(episodeList, i)\n",
        "      totalScore += score\n",
        "    returnScore = totalScore/len(wordSentences)\n",
        "    print(returnScore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuu7HHymo0_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2VecModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, syn0, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(Word2VecModel, self).__init__()\n",
        "\n",
        "        # set class variables\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_length = 140\n",
        "        self.syn0 = syn0\n",
        "\n",
        "        # define model layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # self.sig = nn.Softmax(dim=1)\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.08\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.syn0))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
        "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        self.lstm.bias_ih_l0.data.zero_()\n",
        "        self.lstm.bias_hh_l0.data.zero_()\n",
        "\n",
        "        # self.fc.bias.data.zero_()\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        # self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.normal_(0.0, (1.0 / np.sqrt(self.fc.in_features)))\n",
        "\n",
        "\n",
        "    def forward(self, nn_input, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state\n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = nn_input.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.dropout(self.embedding(nn_input))\n",
        "        # lstm_out, hidden = self.lstm(embeds.view(len(nn_input), self.window_length, -1), hidden)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        sig_out = self.fc(lstm_out)\n",
        "        # out = self.fc1(out)\n",
        "        # print(sig_out.shape)\n",
        "\n",
        "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
        "        sig_out = sig_out[:, -1]\n",
        "\n",
        "        # sig_out = F.log_softmax(sig_out, dim=1)\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "\n",
        "        # initialize hidden state with zero weights, and move to GPU if available\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (torch.cuda.is_available()):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKw4A119o7jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "class W2VVanilla(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, syn0, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(W2VVanilla, self).__init__()\n",
        "\n",
        "        # set class variables\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_length = 140\n",
        "        self.syn0 = syn0\n",
        "\n",
        "        # define model layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.syn0))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # self.sig = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, nn_input, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state\n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = nn_input.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.dropout(self.embedding(nn_input))\n",
        "        # lstm_out, hidden = self.lstm(embeds.view(len(nn_input), self.window_length, -1), hidden)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        sig_out = self.fc(lstm_out)\n",
        "        # out = self.fc1(out)\n",
        "        # print(sig_out.shape)\n",
        "\n",
        "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
        "        sig_out = sig_out[:, -1]\n",
        "\n",
        "        # sig_out = F.log_softmax(sig_out, dim=1)\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        if (torch.cuda.is_available()):\n",
        "            hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
        "        else:\n",
        "            hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlBYBIwko-ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class W2VGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, syn0, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(W2VGRU, self).__init__()\n",
        "\n",
        "        # set class variables\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_length = 140\n",
        "        self.syn0 = syn0\n",
        "\n",
        "        # define model layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.syn0))\n",
        "        self.lstm = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        # self.sig = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, nn_input, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state\n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = nn_input.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.dropout(self.embedding(nn_input))\n",
        "        # lstm_out, hidden = self.lstm(embeds.view(len(nn_input), self.window_length, -1), hidden)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        sig_out = self.fc(lstm_out)\n",
        "        # out = self.fc1(out)\n",
        "        # print(sig_out.shape)\n",
        "\n",
        "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
        "        sig_out = sig_out[:, -1]\n",
        "\n",
        "        # sig_out = F.log_softmax(sig_out, dim=1)\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        if (torch.cuda.is_available()):\n",
        "            hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
        "        else:\n",
        "            hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek4fNrSTg7Ik",
        "colab_type": "code",
        "outputId": "aa6176b9-8d6d-490b-c5e5-544e0356ba9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import datetime\n",
        "# if you are running in uploaded model\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = ['trained_w2v_gru', 'trained_w2v_vanilla', 'trained_lstm']\n",
        "print(onlyfiles)\n",
        "gen_length = 700\n",
        "prime_word = 'scene' # name for starting the script\n",
        "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
        "\n",
        "for i in onlyfiles:\n",
        "  trained_rnn = torch.load('./models/'+i+'.pt')\n",
        "  generated_script = generate(trained_rnn, vocab_to_int[prime_word + ''], int_to_vocab, 10, gen_length)\n",
        "  print(i)\n",
        "  print(generated_script)\n",
        "  bleu(generated_script=generated_script)\n",
        "  selfBleu(generated_script=generated_script)\n",
        "# generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100)\n",
        "# print(generated_script)\n",
        "\n",
        "\n",
        "# f =  open(\"generated_script\" + str(datetime.datetime.now()) + \".txt\",\"w\")\n",
        "# f.write(generated_script)\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['trained_w2v_gru', 'trained_w2v_vanilla', 'trained_lstm']\n",
            "trained_w2v_gru\n",
            "howard. howard leonard. leonard hey ames williams leonard. leonard hey guys are not going to be prepared to do you want to go out with raj. raj so if you re leonard. leonard you re kidding you re howard. howard what do it provided leonard. leonard yeah well you don have to go out of your penny. penny what do you want to go out of the handbook of the leonard. leonard what leonard. leonard what you re talking about sheldon. sheldon yeah well what you do you want you to meet me hopefully you re both sheldon. sheldon no no no no no it was like it to say that you re going to talk about leonard. leonard oh you know what do you want to make me blink to go to sheldon. sheldon oh well we re not going to be staying at leonard. leonard sheldon. sheldon no no no it is not gonna be disappointed that was so much of leonard. leonard what are you sheldon. sheldon subliminal messaging scout to be alone with my leonard. leonard well if it was wubber to penny. penny okay what did you want to do you think sheldon. sheldon what leonard. leonard what do you want to do you have any plans to meet sheldon. sheldon sheldon. sheldon yes what the fwig cortex in raj. raj what sheldon. sheldon oh you re kidding you want to do you want to make me feel better than you re going through leonard. leonard sheldon. sheldon no it not just gonna make me feel better than the sheldon. sheldon well you re being howard. howard what are you doing penny. penny yeah well what do you think you want to do penny. penny you re kidding you re upset you don want to do anything for you to meet him you re being sheldon. sheldon you want to talk to sheldon. sheldon no no no no no no no no no no it not not necessary to be my maid of unruh radiation bonobo dimension and it was wubber to leonard. leonard you think what do you mean you re howard. howard what did you want me to do sheldon. sheldon you are being leonard. leonard what leonard. leonard what did you get loom leonard. leonard no it was going on the floor tasting sheldon. sheldon no no no no it not like sheldon. sheldon you re kidding you guys are you kidding what do you think of semiotics starfleet academy leonard. leonard yeah well it not like that existed you re just saying what happened to the leonard. leonard what did you get raj. raj oh my god that what leonard. leonard oh you know what are you talking to him scene the leonard. leonard hey ames what brings sheldon. sheldon it okay but you know what you re doing here we have to do sheldon. sheldon you re right now we ve been in common for theo cons but painted leonard. leonard what are you raj. raj no no not going for to talk about the sheldon. sheldon oh you re just going to be on the floor amy. amy oh hey guys don need sheldon. sheldon no not that noodle you ve got to leonard. leonard what are you sheldon. sheldon subliminal messaging and not being patently rude and want to go home down and let go to the movies you know if you want to go to leonard. leonard well you know what you do do you think overdressed in your leonard. leonard no no no not going to make me feel better than you don want to go to the leonard. leonard what do you want to go home with sheldon. sheldon well what do you think of my heartwarming to the chase the universe and it not like sheldon. sheldon yes but it is penny. penny oh come on the dung penny. penny yeah sure what happened to say you re not gonna do something sensible of the collective terranean sheldon. sheldon well we should be alone raj. raj penny. penny knock knock knock knock knock knock solemnly knock knock knock knock knock knock knock leonard. leonard knock\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6327850161632641\n",
            "0.6392500382047317\n",
            "trained_w2v_vanilla\n",
            "penny. penny bernadette. bernadette oh you know it you know that you think it smoking in my leonard. leonard oh hey honey sorry you re welcome with the record unit penny. penny you re welcome with strict bernadette. bernadette you know cwogziwwa to be it in the sky slipper court der and then penny. penny you know that not supposed to be late in the way we could be losing and the penny. penny oh sorry sorry you ll be able to burp yourself to the dentist penny. penny well it is it penny. penny oh you re sheldon. sheldon you re welcome to the bathroom slipper of our relationship bernadette. bernadette hey guys you re losing on seniority horse and the inexplicable and you re losing sheldon. sheldon well that was in my life drool bonnet and then you do to see bernadette. bernadette you don have any idea that in the mood of the sheldon. sheldon you know what about howard. howard you know what do sheldon. sheldon oh it is simulation on the leonard. leonard hey you re losing on penny. penny oh you re welcome in seniority and gruesome you know tor of the hoi penny. penny oh that is it was it doesn you re gonna be the processional bernadette. bernadette you know bernadette. bernadette well that tricky it the estrogen of the leonard. leonard howard. howard hey guys are this bernadette. bernadette did you say that doesn you have it in the sky slipper court and then you re not getting shaking raj. raj hey sorry howard. howard hey sorry honey you re clearly with seniority honours disorder ha ha the door knock knock knock knock knock leonard. leonard knock knock knock penny. penny knock knock knock amy. amy knock knock knock knock knock penny. penny knock knock knock penny. penny knock knock knock penny. penny knock knock amy. amy knock knock knock sheldon. sheldon knock knock sheldon. sheldon knock knock amy. amy knock knock knock knock penny. penny knock knock penny. penny knock knock knock penny. penny knock knock knock knock sheldon. sheldon knock knock penny. penny knock knock knock knock penny. penny knock knock knock knock knock amy. amy knock knock knock knock knock knock penny. penny knock knock knock amy. amy knock knock penny. penny knock knock knock penny. penny knock knock knock knock amy. amy knock knock knock knock knock knock sheldon. sheldon knock knock knock penny. penny knock knock knock penny. penny knock knock knock knock penny. penny knock knock amy. amy knock knock knock knock knock amy. amy knock knock knock amy. amy knock the howard. howard oh hey honey sheldon. sheldon hi honey you re welcome on the door penny. penny you re welcome on the other the bernadette. bernadette oh uh honey sheldon. sheldon hey guys sorry hey guys hey sheldon. sheldon hey guys you know that doesn you know that doesn it was in the other of my bernadette. bernadette hey honey you re gonna make bernadette. bernadette sheldon. sheldon honey you re welcome on the stairs slipper of sheldon. sheldon no that just she wrote sheldon. sheldon it is that you re saying we re losing and the penny. penny leonard. leonard hi you don think it you re gonna make you to cuddle it and you know that is it was in sheldon. sheldon well you know bernadette. bernadette you re welcome with strict moustache disorder and tetanus to say bernadette. bernadette well that is my sheldon. sheldon sorry no you re losing battles in my penny. penny oh hey sheldon. sheldon oh thank you re gonna be with the smartest of my life bernadette. bernadette you know sheldon. sheldon it is so sheldon. sheldon you re welcome in the sky and the international space penny. penny oh hey honey sorry you think leonard. leonard sheldon. sheldon hey you re not happy you could have been filed for my penny. penny well you seem to be fair for my mom you could you give it he was in the sky slipper of coffee raj. raj oh so did you want to be burden preserved in the workplace industry court to say why you want\n",
            "0.6442082066422797\n",
            "0.7389416624679855\n",
            "trained_lstm\n",
            "raj. raj leonard. leonard is that the only way we could be in the mood of this year and you re gonna be leonard. leonard yeah you don want to do penny. penny you re right it not like raj. raj well you re not doing leonard. leonard well you re not going to go back with the apartment raj. raj raj. raj sheldon. sheldon penny. penny sheldon. sheldon sheldon. leonard. leonard oh my god you got to get rid of sheldon. sheldon that is the best way you know it like sheldon. sheldon oh well that was the one who told me to go to my room and then he was just trying to have to get married to the movies heartbeat and then he was just trying to be the maid that is the best time you can just get sheldon. sheldon well that not true you re going to do with sheldon. sheldon you want to talk to leonard. leonard oh you re sheldon. sheldon oh that not sheldon. sheldon penny. penny okay well we ll be in the middle of the penny. penny yeah but that the first time and you re not helping me to the howard. howard you re not helping me the best way to get the whole way you know what going raj. raj well it not the same way we can be friends of penny. penny oh you know what the sheldon. sheldon you know what going howard. howard you know how much you want to do howard. howard you know what sheldon. sheldon you are right it was like you can do penny. penny okay you know you re not helping it up with penny. penny oh you know what going sheldon. sheldon you re right it is so howard. howard oh that not fair it just so much for you to do leonard. leonard well you can go in the bernadette. bernadette you re being deliberately to sheldon. sheldon well it not to be sheldon. sheldon leonard. leonard sheldon. sheldon you re kidding to be clear for sheldon. sheldon well you re right there is penny. penny knock knock penny. penny penny. penny penny. penny knock knock penny. leonard. leonard what do you mean that you can do howard. howard sheldon. sheldon no not going out with the same basic to the galaxy penny. penny okay so what do we say that you re not doing sheldon. sheldon you re not in the mood and you know how to fix novelty and the shifts is the one who had to go to the movies heartbeat to leonard. leonard well that is not what you said you can get the sheldon. sheldon no not really good to meet sheldon. sheldon well you re gonna do raj. raj no not just so much about penny. penny well you re right there are the right thing is it possible that way you re not going to be the maid of your sheldon. sheldon oh well you re not going to get it in the middle of the penny. penny you re right it was so bad emly you re in the sheldon. sheldon you don want to do sheldon. sheldon no not really visigoths and he was trying to go to the sheldon. sheldon you want to go to the future raj. raj what are you howard. howard well what are you doing penny. penny okay well you know what going with the guy who has sex and you ll be raj. raj you don have to go to the movies sheldon. sheldon you can be right back and you re just gonna get it in your leonard. leonard okay what the penny. penny okay so we can do sheldon. sheldon you re right there is no way to be part of penny. penny you re right it was like sheldon. sheldon you re leonard. leonard no you re not helping me in the middle of the sheldon. sheldon is that the only one who is sheldon. sheldon well we should just go to the raj. raj you re sheldon. sheldon no you re not going out with penny. penny okay so you\n",
            "0.6715209512208632\n",
            "0.6696005031414428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM-HNLz7_xr6",
        "colab_type": "code",
        "outputId": "24bb68d0-d65f-4d53-aaa4-294efe406fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "mypath = \"./models\"\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(onlyfiles)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['trained_vanilla_sq_7.pt', 'trained_vanilla_sq_5.pt', 'trained_GRU_sq_6.pt', 'trained_GRU_sq_5.pt', 'trained_lstm_sq_7.pt', 'trained_vanilla_sq_6.pt', 'trained_GRU_sq_3.pt', 'trained_GRU_sq_7.pt', 'trained_lstm_sq_6.pt', 'trained_lstm_sq_3.pt', 'trained_vanilla_sq_3.pt', 'trained_lstm_sq_5.pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GHbP00U_23m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}