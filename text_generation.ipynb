{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsjIE9fpXsWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure upload below before run\n",
        "# upload models.py torch_utils.py helper.py preprocess.pkl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RozDWanlfHmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helper\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_utils import batch_data, train_rnn\n",
        "from models import RNN\n",
        "import time\n",
        "import os\n",
        "# hyperparameters\n",
        "sequence_length = 6\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 0.002\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRnbAtSLZdHq",
        "colab_type": "code",
        "outputId": "66673d0c-eeac-45ce-8238-5830b743494b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# def load_preprocess():\n",
        "#     \"\"\"\n",
        "#     Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "#     \"\"\"\n",
        "#     return pickle.load(open('preprocess.pkl', mode='rb'))\n",
        "import os\n",
        "import importlib\n",
        "importlib.reload(helper)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'helper' from '/content/helper.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6EsBGCKXwxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load data\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
        "vocab_size = len(vocab_to_int)\n",
        "output_size = len(vocab_to_int)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKddLoK-XyRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for a GPU\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('No GPU found. Please use a GPU to train your neural network.')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGfuj0j8Xzyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model and move to gpu if available\n",
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.25)\n",
        "if train_on_gpu:\n",
        "    rnn.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-KFrRWX1KA",
        "colab_type": "code",
        "outputId": "63f62f5b-b4f7-43e8-8fc9-d8893b290d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# set decay, optimizer, and loss\n",
        "decay_rate = learning_rate / num_epochs\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9, weight_decay=decay_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "# train the model\n",
        "saved_model_name = 'trained_GRU'\n",
        "trained_rnn, loss_history = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, train_loader, show_every_n_batches, saved_model_name)\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10 epoch(s)...\n",
            "epoch  1\n",
            "Epoch:    1/10    Loss: 5.907130914688111 Decrease Rate: inf \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 5.1510430479049685 Decrease Rate: 0.7560878667831421 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.940020363807678 Decrease Rate: 0.21102268409729064 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.830107000350952 Decrease Rate: 0.10991336345672575 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.775876293182373 Decrease Rate: 0.05423070716857925 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.714502788066864 Decrease Rate: 0.0613735051155091 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.65915013885498 Decrease Rate: 0.05535264921188343 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.620100061416626 Decrease Rate: 0.03905007743835398 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.581644585609436 Decrease Rate: 0.03845547580719 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.535595349311828 Decrease Rate: 0.046049236297608154 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.546127364158631 Decrease Rate: -0.01053201484680244 \n",
            "\n",
            "Epoch:    1/10    Loss: 4.525831464767456 Decrease Rate: 0.009763884544372381 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    1/10    Loss: 4.496655424118042 Decrease Rate: 0.029176040649414148 \n",
            "\n",
            "Model Trained and Saved\n",
            "epoch  2\n",
            "Epoch:    2/10    Loss: 4.357476133170319 Decrease Rate: 0.1391792909477223 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.299415291786194 Decrease Rate: 0.05806084138412526 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.27245788192749 Decrease Rate: 0.026957409858703762 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.266365487575531 Decrease Rate: 0.00609239435195974 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.249173610687256 Decrease Rate: 0.017191876888274393 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.248967470169068 Decrease Rate: 0.00020614051818856893 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.237168583393097 Decrease Rate: 0.01179888677597063 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.269640743732452 Decrease Rate: -0.032472160339355405 \n",
            "\n",
            "Epoch:    2/10    Loss: 4.24218049287796 Decrease Rate: -0.005011909484863075 \n",
            "\n",
            "Epoch:    2/10    Loss: 4.234896923542022 Decrease Rate: 0.002271659851074581 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    2/10    Loss: 4.264609172821045 Decrease Rate: -0.029712249279022274 \n",
            "\n",
            "Epoch:    2/10    Loss: 4.252650894165039 Decrease Rate: -0.017753970623016535 \n",
            "\n",
            "Epoch:    2/10    Loss: 4.227802312850952 Decrease Rate: 0.007094610691070535 \n",
            "\n",
            "Model Trained and Saved\n",
            "epoch  3\n",
            "Epoch:    3/10    Loss: 4.094897413395912 Decrease Rate: 0.13290489945504014 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    3/10    Loss: 4.041790706634521 Decrease Rate: 0.0531067067613904 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    3/10    Loss: 4.012185156345367 Decrease Rate: 0.029605550289153904 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    3/10    Loss: 4.030218705177307 Decrease Rate: -0.01803354883193986 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.028695255756378 Decrease Rate: -0.016510099411010337 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.0443618144989015 Decrease Rate: -0.0321766581535341 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.0408347749710085 Decrease Rate: -0.028649618625641082 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.042001877307892 Decrease Rate: -0.029816720962524634 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.045625037670136 Decrease Rate: -0.03343988132476827 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.0447878966331485 Decrease Rate: -0.032602740287781096 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.069344118595123 Decrease Rate: -0.057158962249755696 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.051085391044617 Decrease Rate: -0.0389002346992493 \n",
            "\n",
            "Epoch:    3/10    Loss: 4.055631332874298 Decrease Rate: -0.04344617652893046 \n",
            "\n",
            "epoch  4\n",
            "Epoch:    4/10    Loss: 3.907510702727271 Decrease Rate: 0.10467445361809657 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    4/10    Loss: 3.8606819505691528 Decrease Rate: 0.0468287521581181 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    4/10    Loss: 3.8784858870506285 Decrease Rate: -0.017803936481475713 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.870613656997681 Decrease Rate: -0.009931706428528031 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.899961314678192 Decrease Rate: -0.03927936410903943 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.8800395097732543 Decrease Rate: -0.019357559204101538 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.885795458316803 Decrease Rate: -0.02511350774765031 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.9145151977539063 Decrease Rate: -0.053833247184753574 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.912851719379425 Decrease Rate: -0.052169768810272465 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.9211291751861572 Decrease Rate: -0.06044722461700447 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.9316258873939516 Decrease Rate: -0.07094393682479883 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.9076298222541808 Decrease Rate: -0.04694787168502801 \n",
            "\n",
            "Epoch:    4/10    Loss: 3.9298285388946534 Decrease Rate: -0.06914658832550069 \n",
            "\n",
            "epoch  5\n",
            "Epoch:    5/10    Loss: 3.791729401902541 Decrease Rate: 0.0689525486666116 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    5/10    Loss: 3.740641489982605 Decrease Rate: 0.051087911919936335 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    5/10    Loss: 3.7529280200004576 Decrease Rate: -0.012286530017852737 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.7638709630966187 Decrease Rate: -0.02322947311401391 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.758540687084198 Decrease Rate: -0.017899197101593156 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.7826612186431885 Decrease Rate: -0.04201972866058368 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.794142337799072 Decrease Rate: -0.053500847816467356 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.7891369676589965 Decrease Rate: -0.04849547767639173 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.812818022727966 Decrease Rate: -0.0721765327453614 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.7938465695381165 Decrease Rate: -0.05320507955551168 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.8453791036605836 Decrease Rate: -0.10473761367797874 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.8495848960876464 Decrease Rate: -0.1089434061050416 \n",
            "\n",
            "Epoch:    5/10    Loss: 3.8737055726051333 Decrease Rate: -0.13306408262252845 \n",
            "\n",
            "epoch  6\n",
            "Epoch:    6/10    Loss: 3.700395394011155 Decrease Rate: 0.04024609597144968 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    6/10    Loss: 3.656011924743652 Decrease Rate: 0.04438346926750292 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    6/10    Loss: 3.6662634119987487 Decrease Rate: -0.010251487255096503 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.6678456573486327 Decrease Rate: -0.01183373260498044 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.686186689376831 Decrease Rate: -0.030174764633178697 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.69397509431839 Decrease Rate: -0.037963169574737776 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.7213399271965026 Decrease Rate: -0.06532800245285042 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.7392165918350218 Decrease Rate: -0.08320466709136953 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.730675705909729 Decrease Rate: -0.07466378116607686 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.73915625 Decrease Rate: -0.08314432525634796 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.738609874725342 Decrease Rate: -0.08259794998168957 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.8027055463790895 Decrease Rate: -0.1466936216354373 \n",
            "\n",
            "Epoch:    6/10    Loss: 3.77311293554306 Decrease Rate: -0.11710101079940793 \n",
            "\n",
            "epoch  7\n",
            "Epoch:    7/10    Loss: 3.6356347523929466 Decrease Rate: 0.020377172350705575 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    7/10    Loss: 3.5913821682929994 Decrease Rate: 0.044252584099947256 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    7/10    Loss: 3.60694828081131 Decrease Rate: -0.015566112518310415 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.6078118796348573 Decrease Rate: -0.01642971134185789 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.6190299868583677 Decrease Rate: -0.027647818565368354 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.6534696040153505 Decrease Rate: -0.06208743572235109 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.642511288166046 Decrease Rate: -0.05112911987304658 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.67524280834198 Decrease Rate: -0.08386064004898053 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.669417278289795 Decrease Rate: -0.07803510999679553 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.677084300041199 Decrease Rate: -0.0857021317481994 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.7085622701644896 Decrease Rate: -0.1171801018714902 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.705354830741882 Decrease Rate: -0.11397266244888282 \n",
            "\n",
            "Epoch:    7/10    Loss: 3.702560027599335 Decrease Rate: -0.1111778593063355 \n",
            "\n",
            "epoch  8\n",
            "Epoch:    8/10    Loss: 3.574805683244003 Decrease Rate: 0.016576485048996226 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    8/10    Loss: 3.53454655790329 Decrease Rate: 0.04025912534071319 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    8/10    Loss: 3.5567080969810485 Decrease Rate: -0.02216153907775853 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.574474058151245 Decrease Rate: -0.039927500247955106 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.556644865989685 Decrease Rate: -0.022098308086395235 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.5808436918258666 Decrease Rate: -0.046297133922576617 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.6031235818862917 Decrease Rate: -0.0685770239830017 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.6068896565437316 Decrease Rate: -0.07234309864044164 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.60569659948349 Decrease Rate: -0.0711500415802 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.6392123217582704 Decrease Rate: -0.10466576385498039 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.6588749504089355 Decrease Rate: -0.12432839250564554 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.646396385192871 Decrease Rate: -0.11184982728958115 \n",
            "\n",
            "Epoch:    8/10    Loss: 3.663778519153595 Decrease Rate: -0.129231961250305 \n",
            "\n",
            "epoch  9\n",
            "Epoch:    9/10    Loss: 3.528935848867129 Decrease Rate: 0.005610709036160788 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    9/10    Loss: 3.5010645513534544 Decrease Rate: 0.027871297513674786 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    9/10    Loss: 3.4995796704292297 Decrease Rate: 0.0014848809242247185 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:    9/10    Loss: 3.516193323612213 Decrease Rate: -0.016613653182983246 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.524467011928558 Decrease Rate: -0.024887341499328475 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.5387357835769655 Decrease Rate: -0.039156113147735816 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.5441076874732973 Decrease Rate: -0.044528017044067614 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.568818284034729 Decrease Rate: -0.06923861360549921 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.565801269054413 Decrease Rate: -0.06622159862518329 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.5781062812805176 Decrease Rate: -0.07852661085128787 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.600390027999878 Decrease Rate: -0.10081035757064827 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.6280428714752198 Decrease Rate: -0.12846320104599007 \n",
            "\n",
            "Epoch:    9/10    Loss: 3.637166244029999 Decrease Rate: -0.1375865736007693 \n",
            "\n",
            "epoch  10\n",
            "Epoch:   10/10    Loss: 3.4707149361356007 Decrease Rate: 0.028864734293629013 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:   10/10    Loss: 3.473778579235077 Decrease Rate: -0.0030636430994763764 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.469905067920685 Decrease Rate: 0.0008098682149157987 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:   10/10    Loss: 3.4825443110466003 Decrease Rate: -0.012639243125915467 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.4685726070404055 Decrease Rate: 0.0013324608802793847 \n",
            "\n",
            "Model Trained and Saved\n",
            "Epoch:   10/10    Loss: 3.5052522654533385 Decrease Rate: -0.03667965841293297 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.5123687238693235 Decrease Rate: -0.04379611682891804 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.5009949612617492 Decrease Rate: -0.03242235422134376 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.5256492867469786 Decrease Rate: -0.05707667970657315 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.5506823186874388 Decrease Rate: -0.08210971164703329 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.5713547496795655 Decrease Rate: -0.10278214263915997 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.586244482040405 Decrease Rate: -0.11767187499999965 \n",
            "\n",
            "Epoch:   10/10    Loss: 3.6027429695129394 Decrease Rate: -0.1341703624725339 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqAIcrDQ2LCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkPTpeEPY2HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have a pretrained model, you can start from here,\n",
        "# make sure load helper.py trained_rnn_new.pt preprocess.pkl models.py\n",
        "import torch\n",
        "import helper\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "\n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "\n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "\n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "\n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)\n",
        "\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "\n",
        "    gen_sentences = ' '.join(predicted)\n",
        "\n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token, key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "\n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZGJZL8U7wBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "# if you are running in uploaded model\n",
        "trained_rnn = helper.load_model('./trained_rnn_new')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2BvEIKZO96P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# with open('preprocess.pkl', 'rb') as f:\n",
        "#     data = pickle.load(f)\n",
        "# print(data[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkTskNB6BW5",
        "colab_type": "code",
        "outputId": "185d03eb-3053-4f15-c6aa-c18e5c318c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# run the cell multiple times to get different results!\\\n",
        "import datetime\n",
        "gen_length = 400\n",
        "prime_word = 'scene' # name for starting the script\n",
        "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
        "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ''], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "# generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100)\n",
        "print(generated_script)\n",
        "\n",
        "\n",
        "f =  open(\"generated_script\" + str(datetime.datetime.now()) + \".txt\",\"w\")\n",
        "f.write(generated_script)\n",
        "f.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scene: the apartment. sheldon: i’m sorry. i just wanted to talk to him. leonard: oh, i can’t tell you what? amy: i don’t know what you meant, i have no idea about it. penny: okay. i guess i was thinking of your own culture and the pope who has crossed his own desk? howard: i don’t have coffee. sheldon(on video): hey, i don’t know why i’m on a little. leonard: oh. leonard: i don’t know. i don’t want to be a big step, i have to go to a bar. sheldon: i don’t know. penny: well, i think i have to go. penny: yeah. howard: yeah, but i was hoping for you to be a good thing. i mean, the truth is, but i think you should do this. penny: okay, look at that. sheldon: you don’t know what i said to me, i have to go to the bathroom and make me a little awkward. leonard: oh. well, i was hoping we should be able to make you feel like a slob, and i’m gonna go out with the new subcontinent of the university. sheldon: well, that’s not true. penny: yeah, you can. leonard: yeah, you know what i’m going to do is reassure her. leonard: i don’t know. penny: okay, let’s get this. sheldon(voice): oh, hey. penny: hey, i just wanted you to know that. sheldon: you want to go? penny: no, no, no, no, no. i have to be cinderella. leonard: you want to do that? leonard: well, i don’t think so. sheldon: oh. i don’t think so, but i was going to be in a bar? raj: no, i can’t. i have no interest of the airport, and i was thinking of starting. sheldon: no, no, no, i don’t know what you said? leonard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTdKtbtU1ybF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}